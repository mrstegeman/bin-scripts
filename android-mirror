#!/usr/bin/env python3

"""
This script will mirror the entire Android SDK repository.

It will pull in all known legacy manifests (used by older SDK installations),
as well as the latest. Every package listed in those manifests will be
downloaded. All add-on repositories will also be parsed. The only packages that
will not be mirrored are those that are not located on the main repository
site.
"""

import hashlib
import os
import re
import sys
import xml.etree.ElementTree as ET
from urllib.error import HTTPError
from urllib.request import Request, urlopen

BASE_URL = 'https://dl-ssl.google.com/android/repository/'
REPOS = {
    'repository': {
        'keep': [
            '',
            '-5',
            '-6',
            '-7',
            '-8',
            '-10',
            '-11',
        ],
        'start': 12,
        'ns': 'sdk:',
    },
    'repository2': {
        'keep': [],
        'start': 1,
        'ns': '',
    },
    'addons_list': {
        'keep': [
            '-1',
            '-2',
        ],
        'start': 3,
        'ns': 'sdk:',
    }
}

if sys.stdout.isatty() and os.name != 'nt':
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    RESET = '\033[39m'
else:
    RED = ''
    GREEN = ''
    YELLOW = ''
    RESET = ''

__FETCHED = set()


def hash_file(fname):
    """
    Compute the SHA-1 hash of the given file.

    fname -- File to hash
    """
    try:
        with open(fname, 'rb') as f:
            sha1 = hashlib.sha1()
            while True:
                data = f.read(4096)
                if not data:
                    break
                sha1.update(data)

            return sha1.hexdigest()
    except (OSError, IOError):
        return None


def download_to_file(url, fname, cksum=None):
    """
    Download (GET) the given URL to a file.

    url -- The URL to GET
    fname -- The name of the file to save the content to
    cksum -- (Optional) Checksum to verify against
    """
    try:
        if os.path.isfile(fname):
            if cksum is not None and cksum == hash_file(fname):
                __FETCHED.add(fname)
                return True
            else:
                os.unlink(fname)

        dname = os.path.dirname(fname)
        if not os.path.isdir(dname):
            try:
                os.makedirs(dname)
            except OSError:
                print('{}FAILED!{}'.format(RED, RESET))
                return False

        print('{}Fetching: {}{} ... '.format(YELLOW, RESET, url), end='')
        with urlopen(url) as response:
            with open(fname, 'wb') as f:
                while True:
                    chunk = response.read(4096)
                    if not chunk:
                        break

                    f.write(chunk)

        if cksum is not None and cksum != hash_file(fname):
            os.unlink(fname)
            print('{}CHECKSUM FAILED!{}'.format(RED, RESET))
            return False

        __FETCHED.add(fname)
        print('{}SUCCESS!{}'.format(GREEN, RESET))
        return True
    except (OSError, IOError, HTTPError) as e:
        print('{}FAILED!{}'.format(RED, RESET))
        return False


def find_latest_repo(base, start):
    """
    Find the latest iteration of the given repo base.

    base -- The base repo url
    start -- The starting iteration point
    """
    current = start
    while True:
        url = '{}{}-{}.xml'.format(BASE_URL, base, current)

        request = Request(url, method='HEAD')
        try:
            with urlopen(request) as response:
                if response.getcode() != 200:
                    return current - 1

            current += 1
        except HTTPError:
            return current - 1


def parse_repo(repo, ns, output_dir, base_url):
    """
    Parse an individual XML file.

    repo -- The XML file to parse
    ns -- The namespace prefix for XML elements, i.e. 'sdk:'
    output_dir -- The output directory to save files to
    base_url -- The base URL to start from
    """
    print('{}Parsing repository: {}{}'.format(YELLOW, RESET, repo))
    context = ET.iterparse(repo, events=['start-ns', 'end'])
    namespaces = {}
    root = None
    for event, el in context:
        if event == 'start-ns':
            namespaces[el[0]] = el[1]
        else:
            root = el

    for node in root.findall('.//{}url/..'.format(ns), namespaces):
        url = node.find('./{}url[1]'.format(ns), namespaces)
        if url is None:
            continue

        url = url.text

        cksum = node.find('./{}checksum[1]'.format(ns), namespaces)
        if cksum is not None:
            cksum = cksum.text

        if url.startswith('http'):
            if url.startswith(base_url):
                url = url[len(base_url):]
            else:
                print('{}Not fetching URL: {}{}'.format(YELLOW, RESET, url))
                continue

        full_url = base_url + url
        fname = os.path.join(output_dir, *url.split('/'))

        download_to_file(full_url, fname, cksum=cksum)

        if url.endswith('.xml'):
            dname = os.path.dirname(url) + '/'
            new_dir = os.path.join(output_dir, *dname.split('/'))
            parse_repo(fname, ns, new_dir, base_url + dname)


def cleanup(base_dir):
    """
    Clean up the directory, i.e. remove any files that were not in the repo.

    base_dir -- Directory to search
    """
    for root, dirs, files in os.walk(base_dir, topdown=False):
        for name in files:
            fname = os.path.join(root, name)
            if fname not in __FETCHED:
                print('{}Deleting: {}{}'.format(YELLOW, RESET, fname))
                os.unlink(fname)

        for name in dirs:
            dname = os.path.join(root, name)
            if not os.listdir(dname):
                print('{}Deleting: {}{}'.format(YELLOW, RESET, dname))
                os.rmdir(dname)


def main():
    """Download and parse known repository files."""
    if len(sys.argv) < 2:
        print('Usage: {} directory'.format(__file__))
        sys.exit(1)

    output_dir = re.sub(r'[/\\]*$', os.sep, sys.argv[1])
    repos_to_parse = []

    if not os.path.isdir(output_dir):
        try:
            os.makedirs(output_dir, mode=0o755)
        except OSError as e:
            print('{}Failed to create output directory: {}{}'
                  .format(RED, RESET, e))
            sys.exit(1)

    for repo_base, data in REPOS.items():
        for repo in data['keep']:
            url = '{}{}{}.xml'.format(BASE_URL, repo_base, repo)
            fname = os.path.join(output_dir,
                                 '{}{}.xml'.format(repo_base, repo))

            if download_to_file(url, fname):
                repos_to_parse.append((fname, data['ns']))

        num = find_latest_repo(repo_base, data['start'])
        url = '{}{}-{}.xml'.format(BASE_URL, repo_base, num)
        fname = os.path.join(output_dir, '{}-{}.xml'.format(repo_base, num))
        if download_to_file(url, fname):
            repos_to_parse.append((fname, data['ns']))
        else:
            sys.exit(1)

    for (repo, ns) in repos_to_parse:
        parse_repo(repo, ns, output_dir, BASE_URL)

    cleanup(output_dir)


if __name__ == '__main__':
    main()
